#summary How to review an XForms 1.1 Test Suite chapter.
#labels Phase-QA,HowTo,Status

= Introduction =

As we get closer to completing XForms 1.1 support, we need to keep track of where the gaps are in our coverage, so that people can get straight to what needs to be done.

= Status files =

We are still in the process of automating our tests, and until that happens, we have two sources of information about which tests pass:

  * manually maintained chapter status files;
  * Selenium-generated chapter status files.

Once the tests are full automated the test-generated files will always be authoritative.

== Manually maintained chapter status files ==

The first source is a set of XML files, one for each test suite chapter, which contains an entry for each test, and an indicator of whether the tests has passed or not. These documents are used to generate an HTML page which summarises the status of the tests in each chapter.

These files are currently maintained manually, with developers updating the documents whenever they pass a test. Whenever an updated chapter file is checked in to SVN, a Buildbot slave will regenerate the HTML status files.

The source XML files are are maintained for each supported. Currently we have [http://ubiquity-xforms.googlecode.com/svn/trunk/testsuite/W3C-XForms-1.1/Edition1/driverPages/Results/FF3/ chapter files for Firefox 3] and [http://ubiquity-xforms.googlecode.com/svn/trunk/testsuite/W3C-XForms-1.1/Edition1/driverPages/Results/IE7/ chapter files for IE 7].

The HTML status pages are available on the Buildbot server, [http://uxf-bb.webbackplane.com:8080/Results/IE7/ResultsTable.html one for IE 7] and [http://uxf-bb.webbackplane.com:8080/Results/FF3/ResultsTable.html one for FF 3].

== Selenium-generated chapter status files ==

Selenium can generate a status file that shows the output of a test run. However, there are currently two problems. The first is that even after breaking the XForms 1.1 test suite file into multiple chapter files, many of them still require human intervention to complete. Secondly, many of the tests have not been written 'defensively', which means they can fail in different environments. The consequence of this is that some of the tests that fail under Selenium are regarded as a 'pass' by developers who run the tests manually, and so the Selenium-generated file cannot be regarded as authoritative.

The generated files are in the same directories as above ( [http://uxf-bb.webbackplane.com:8080/Results/IE7/ one for IE 7] and [http://uxf-bb.webbackplane.com:8080/Results/FF3/ one for FF 3]), with names that follow the following pattern:
{{{
xforms11-Chapter7-ie-results.html
}}}

= Reviewing a chapter =

The first step in reviewing a chapter is to see if there are any tests that the automated system says are passed, but which the developers have not manually marked as passed. This could be for a number of reasons, but the main one will be that the test in question hasn't been run; this can happen because when working on an issue, it will not always be clear what other tests might apply to the features you are working on.

== 'Missing' passing tests ==

Since not all tests will necessarily have been run, then it is possible that there are tests listed in the manually maintained documents that could be passed, but are still marked as failing.

We are currently in the process of updating the manually maintained files to include any new information we've obtained by running the automated tests. I.e., if a test shows green on the Selenium-generated files but red on the manual files, we can safely update the manual file.

To illustrate, if we wanted to check chapter 7 for tests that Selenium has passed, but developers hadn't, we would look at the following two files:

||[http://uxf-bb.webbackplane.com:8080/Results/IE7/html/XF11_07_Results.html Manual test results for IE 7]||
||[http://uxf-bb.webbackplane.com:8080/Results/IE7/xforms11-Chapter7-ie-results.html Selenium test results]||

Anything that is red in the first file, but green in the second can be marked as passed in the manual files.

== Tests that are blocking automation because they should pass (false negatives) ==

Any test that is marked as being green in the manual file, but red in the second is preventing the test-suite from being automated, because it is giving the wrong result. Of course there will be other things that are blocking automation, which we'll look at in the next section, but for 'false negatives' we're looking at tests that are successfull when run manually (and so have a green entry in the manually-generated file), but fail when run in Selenium (having a red entry in the Selenium-generated file).

Some guidance on how to address these kinds of issues is at r2317, which explains how to:

  * indicate that the XForms processor has been loaded and initialised, by adding an `xf:output` to the end of the form;
  * use this `xf:output` in a driver page, so that the test can wait until the XForms processor has been initialised before continuing.